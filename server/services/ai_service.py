import os
import re
import json
import asyncio
from typing import Literal, Optional

from pydantic import BaseModel, Field, ValidationError
from langchain_nvidia_ai_endpoints import ChatNVIDIA
from langchain_core.messages import HumanMessage, SystemMessage


# ============================================================================
# ğŸ“¦ AI ì‘ë‹µ ê²€ì¦ìš© Pydantic ëª¨ë¸ (local-program EditorCommandì™€ 1:1 ëŒ€ì‘)
# ============================================================================

VALID_TYPES = Literal[
    "focus_window",
    "hotkey",
    "type_text",
    "command_palette",
    "open_file",
    "goto_line",
    "open_folder",
    "save_file",
]


class AIDecision(BaseModel):
    """AIê°€ ë°˜í™˜í•´ì•¼ í•˜ëŠ” êµ¬ì¡°í™”ëœ ì˜ì‚¬ê²°ì •"""

    type: VALID_TYPES
    payload: dict
    guidance: str = ""
    should_pause: bool = False
    target_file: Optional[str] = None  # í¸ì§‘ ëŒ€ìƒ íŒŒì¼ëª… (ë¡œì»¬ì´ ì˜¬ë°”ë¥¸ íŒŒì¼ì—ì„œ ì‘ì—…í•˜ë„ë¡)
    expected_content: Optional[str] = None  # í™”ë©´ì— ë³´ì´ëŠ” í˜„ì¬ íŒŒì¼ ë‚´ìš© (ë¡œì»¬ íŒŒì¼ ê²€ì¦ìš©)


# ============================================================================
# ğŸ§  AI Service
# ============================================================================


class AIService:
    MAX_RETRIES = 1  # ê²€ì¦ ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„ íšŸìˆ˜

    def __init__(self):
        # ----------------------------------------------------------------
        # ğŸ¤– ëª¨ë¸ ì„ íƒ (OCR ë²¤ì¹˜ë§ˆí¬ ê¸°ì¤€ ìˆœìœ„)
        # ----------------------------------------------------------------
        # 1ìˆœìœ„: Llama 4 Maverick â€” OCR 82.3%, DocVQA 94.4%, 1M ì»¨í…ìŠ¤íŠ¸
        model_id = "meta/llama-4-maverick-17b-128e-instruct"
        # 2ìˆœìœ„: Nemotron Nano VL â€” OCRBench v2 1ìœ„(92.3%), 128K ì»¨í…ìŠ¤íŠ¸
        # model_id = "nvidia/nemotron-nano-12b-v2-vl"
        # 3ìˆœìœ„: Llama 4 Scout â€” OCR 74.3%, 10M ì»¨í…ìŠ¤íŠ¸, ë” ë¹ ë¦„
        # model_id = "meta/llama-4-scout-17b-16e-instruct"
        # ----------------------------------------------------------------
        self.llm = ChatNVIDIA(
            model=model_id,
            nvidia_api_key=os.getenv("NVIDIA_API_KEY"),
            temperature=0.15,
        )

        # local-program/models/commands.py EditorCommand ìŠ¤í‚¤ë§ˆì™€ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” í”„ë¡¬í”„íŠ¸
        self.system_prompt = """ë„ˆëŠ” ì‹œê°ì¥ì• ì¸ ìˆ˜ê°•ìƒì„ ìœ„í•´ ê°•ì˜ ì˜ìƒ ì† ê°•ì‚¬ì˜ ë™ì‘ì„ ë¶„ì„í•˜ê³  ì—ë””í„°ë¥¼ ì œì–´í•˜ëŠ” AI ì—ì´ì „íŠ¸ì•¼.
ê°•ì˜ í™”ë©´(ì´ë¯¸ì§€)ê³¼ ìë§‰(transcript)ì„ ë¶„ì„í•˜ì—¬ ìˆ˜ê°•ìƒì´ ë”°ë¼í•´ì•¼ í•  ë™ì‘ì„ íŒë‹¨í•˜ê³ , ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´.
JSON ì™¸ì˜ í…ìŠ¤íŠ¸(ì„¤ëª…, ë§ˆí¬ë‹¤ìš´ ë“±)ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆ.

[ëª…ë ¹ì–´ íƒ€ì… + payload ìŠ¤í‚¤ë§ˆ]

1. focus_window â€” ì°½ ì „í™˜
   payload: { "window_title": "Visual Studio Code" }

2. hotkey â€” ë‹¨ì¶•í‚¤ ì‹¤í–‰
   payload: { "keys": ["ctrl", "s"] }

3. type_text â€” ì½”ë“œ/í…ìŠ¤íŠ¸ ì…ë ¥
   payload: { "content": "print('hello')" }

4. command_palette â€” VS Code ëª…ë ¹ íŒ”ë ˆíŠ¸
   payload: { "command": "Go to Line" }

5. open_file â€” íŒŒì¼ ì—´ê¸°
   payload: { "file_path": "C:/project/main.py" }

6. goto_line â€” íŠ¹ì • ë¼ì¸ìœ¼ë¡œ ì´ë™
   payload: { "line_number": 42 }
   (ì„ íƒ) payload: { "line_number": 42, "column": 10 }

7. open_folder â€” í´ë” ì—´ê¸°
   payload: { "folder_path": "C:/project", "new_window": false }

8. save_file â€” íŒŒì¼ ì €ì¥
   payload: { "file_name": null, "folder_path": null }

[ì‘ë‹µ í˜•ì‹ â€” ë°˜ë“œì‹œ ì´ JSONë§Œ ì¶œë ¥]
{
  "type": "ëª…ë ¹ì–´íƒ€ì…",
  "payload": { ... ìœ„ ìŠ¤í‚¤ë§ˆì— ë§ëŠ” í•„ë“œ ... },
  "guidance": "ìŠ¤í¬ë¦°ë¦¬ë”ê°€ ì½ì–´ì¤„ ì¹œì ˆí•œ í•œêµ­ì–´ ì„¤ëª…",
  "should_pause": true ë˜ëŠ” false,
  "target_file": "í¸ì§‘í•  íŒŒì¼ëª… (ì˜ˆ: main.py)",
  "expected_content": "í™”ë©´ì— ë³´ì´ëŠ” í•´ë‹¹ íŒŒì¼ì˜ í˜„ì¬ ì „ì²´ ì½”ë“œ ë‚´ìš© (ìˆëŠ” ê·¸ëŒ€ë¡œ)"
}

[ê·œì¹™]
- typeì€ ìœ„ 8ê°€ì§€ ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•¨
- payloadëŠ” í•´ë‹¹ íƒ€ì…ì˜ ìŠ¤í‚¤ë§ˆë¥¼ ì •í™•íˆ ë”°ë¼ì•¼ í•¨
- guidanceëŠ” ì‹œê°ì¥ì• ì¸ì´ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì¹œì ˆí•˜ê²Œ ì‘ì„±
- should_pause: ê°•ì˜ë¥¼ ì¼ì‹œì •ì§€í•´ì•¼ í•˜ë©´ true, ì•„ë‹ˆë©´ false
- target_file: í¸ì§‘ ëª…ë ¹(type_text, goto_line, hotkey, save_file)ì¼ ë•Œ ë°˜ë“œì‹œ ëŒ€ìƒ íŒŒì¼ëª…ì„ í¬í•¨í•´ì•¼ í•¨ (ì˜ˆ: "main.py", "app.js"). ë¡œì»¬ ì—ì´ì „íŠ¸ê°€ ì˜¬ë°”ë¥¸ íŒŒì¼ì„ ì—´ê³  í¸ì§‘í•˜ê¸° ìœ„í•´ í•„ìˆ˜ì„. í™”ë©´ì—ì„œ ê°•ì‚¬ê°€ ì‘ì—… ì¤‘ì¸ íŒŒì¼ëª…ì„ ì½ì–´ì„œ ë„£ì–´ì¤˜.
- expected_content: í¸ì§‘ ëª…ë ¹ì¼ ë•Œ í™”ë©´ì— ë³´ì´ëŠ” í•´ë‹¹ íŒŒì¼ì˜ í˜„ì¬ ì „ì²´ ì½”ë“œ ë‚´ìš©ì„ ìˆëŠ” ê·¸ëŒ€ë¡œ ë„£ì–´ì¤˜. ë¡œì»¬ ì—ì´ì „íŠ¸ê°€ ê°™ì€ ì´ë¦„ì˜ ë‹¤ë¥¸ íŒŒì¼ì— ì˜ëª» í¸ì§‘í•˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•œ ê²€ì¦ìš©ì„. í™”ë©´ì— ë³´ì´ëŠ” ì½”ë“œë¥¼ ìµœëŒ€í•œ ì •í™•í•˜ê²Œ ì½ì–´ì„œ ë„£ì–´ì¤˜. í™”ë©´ì— ì¼ë¶€ë§Œ ë³´ì´ë©´ ë³´ì´ëŠ” ë¶€ë¶„ë§Œ ë„£ì–´ë„ ë¨.
- í™”ë©´ì— ë³€í™”ê°€ ì—†ê±°ë‚˜ ëª…ë ¹ì´ ë¶ˆí•„ìš”í•˜ë©´ typeì„ "type_text", payloadë¥¼ {"content": ""}, should_pauseë¥¼ falseë¡œ
"""

    # ------------------------------------------------------------------
    # í•µì‹¬ ë©”ì„œë“œ
    # ------------------------------------------------------------------
    async def analyze_and_decide(
        self,
        image_b64: str,
        local_status: str,
        transcript_context: list[str] | None = None,
    ) -> dict:
        """
        NVIDIA NIM VLMìœ¼ë¡œ í™”ë©´ ë¶„ì„ â†’ Pydantic ê²€ì¦ â†’ ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„.

        Returns:
            AIDecisionê³¼ ë™ì¼í•œ êµ¬ì¡°ì˜ dict (type, payload, guidance, should_pause)
        """
        messages = self._build_messages(image_b64, local_status, transcript_context)

        for attempt in range(1 + self.MAX_RETRIES):
            try:
                response = await self.llm.ainvoke(messages)
                raw_text = response.content
                print(f"ğŸ¤– [AI ì›ë¬¸] {raw_text[:300]}")

                raw_json = self._extract_json(raw_text)
                decision = AIDecision.model_validate(raw_json)

                print(
                    f"âœ… [AI ê²°ì •] type={decision.type} "
                    f"payload={json.dumps(decision.payload, ensure_ascii=False)[:100]} "
                    f"pause={decision.should_pause}"
                )
                return decision.model_dump()

            except ValidationError as e:
                if attempt < self.MAX_RETRIES:
                    # ì¬ì‹œë„: ê²€ì¦ ì—ëŸ¬ë¥¼ í”¼ë“œë°±ìœ¼ë¡œ ì œê³µ
                    error_msg = str(e)
                    print(f"âš ï¸ AI ì‘ë‹µ ê²€ì¦ ì‹¤íŒ¨ (ì¬ì‹œë„ {attempt + 1}): {error_msg[:100]}")
                    messages.append(
                        HumanMessage(
                            content=(
                                f"ì‘ë‹µì´ ìŠ¤í‚¤ë§ˆ ê²€ì¦ì— ì‹¤íŒ¨í–ˆì–´. ì—ëŸ¬: {error_msg}\n"
                                "ìœ„ ìŠ¤í‚¤ë§ˆë¥¼ ì •í™•íˆ ë”°ë¼ì„œ JSONë§Œ ë‹¤ì‹œ ì¶œë ¥í•´ì¤˜."
                            )
                        )
                    )
                else:
                    print(f"âŒ AI ì‘ë‹µ ê²€ì¦ ìµœì¢… ì‹¤íŒ¨: {e}")
                    return self._fallback_decision("ì‘ë‹µì´ ì˜¬ë°”ë¥¸ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")

            except (ValueError, json.JSONDecodeError) as e:
                if attempt < self.MAX_RETRIES:
                    print(f"âš ï¸ JSON ì¶”ì¶œ ì‹¤íŒ¨ (ì¬ì‹œë„ {attempt + 1}): {e}")
                    messages.append(
                        HumanMessage(
                            content=(
                                "JSON íŒŒì‹±ì— ì‹¤íŒ¨í–ˆì–´. ë°˜ë“œì‹œ ìˆœìˆ˜ JSONë§Œ ì¶œë ¥í•´. "
                                "ë§ˆí¬ë‹¤ìš´ì´ë‚˜ ì„¤ëª… í…ìŠ¤íŠ¸ ì—†ì´ { ... } ë§Œ ì‘ë‹µí•´ì¤˜."
                            )
                        )
                    )
                else:
                    print(f"âŒ JSON ì¶”ì¶œ ìµœì¢… ì‹¤íŒ¨: {e}")
                    return self._fallback_decision("JSONì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            except Exception as e:
                print(f"âŒ AI ë¶„ì„ ì‹¤íŒ¨: {e}")
                return self._fallback_decision("í™”ë©´ì„ ë¶„ì„í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

        return self._fallback_decision("ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜")

    # ------------------------------------------------------------------
    # ë‚´ë¶€ í—¬í¼
    # ------------------------------------------------------------------
    def _build_messages(
        self,
        image_b64: str,
        local_status: str,
        transcript_context: list[str] | None,
    ) -> list:
        """LLM í˜¸ì¶œìš© ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ êµ¬ì„±"""
        text_parts = [f"í˜„ì¬ ë¡œì»¬ ìƒíƒœ: {local_status}."]

        if transcript_context:
            recent = "\n".join(transcript_context[-5:])
            text_parts.append(f"ìµœê·¼ ê°•ì˜ ìë§‰:\n{recent}")

        text_parts.append("í™”ë©´ì„ ë¶„ì„í•˜ê³  ìˆ˜ê°•ìƒì´ ë”°ë¼í•´ì•¼ í•  ëª…ë ¹ì„ JSONìœ¼ë¡œ ë‚´ë ¤ì¤˜.")

        content = [
            {"type": "text", "text": "\n\n".join(text_parts)},
            {"type": "image_url", "image_url": {"url": image_b64}},
        ]

        return [
            SystemMessage(content=self.system_prompt),
            HumanMessage(content=content),
        ]

    @staticmethod
    def _fallback_decision(reason: str) -> dict:
        """ê²€ì¦/íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì•ˆì „í•œ ê¸°ë³¸ ì‘ë‹µ"""
        return AIDecision(
            type="type_text",
            payload={"content": ""},
            guidance=reason,
            should_pause=True,
        ).model_dump()

    @staticmethod
    def _extract_json(text: str) -> dict:
        """
        AI ì‘ë‹µì—ì„œ ì²« ë²ˆì§¸ JSON ê°ì²´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

        AIê°€ ì—¬ëŸ¬ JSONì„ ì—°ì† ì¶œë ¥í•˜ëŠ” ê²½ìš°({ ... } { ... })
        ì²« ë²ˆì§¸ ì™„ì „í•œ ê°ì²´ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.
        """
        text = text.strip()

        # 1ì°¨: ì „ì²´ê°€ ë‹¨ì¼ JSONì´ë©´ ë°”ë¡œ íŒŒì‹±
        try:
            return json.loads(text)
        except json.JSONDecodeError:
            pass

        # 2ì°¨: ë§ˆí¬ë‹¤ìš´ ì½”ë“œë¸”ë¡ ì¶”ì¶œ
        match = re.search(r"```(?:json)?\s*\n?(.*?)\n?\s*```", text, re.DOTALL)
        if match:
            try:
                return json.loads(match.group(1).strip())
            except json.JSONDecodeError:
                pass

        # 3ì°¨: balanced brace ë§¤ì¹­ â€” ì²« ë²ˆì§¸ { ... } ê°ì²´ë§Œ ì¶”ì¶œ
        result = AIService._extract_first_json_object(text)
        if result is not None:
            return result

        raise ValueError(f"AI ì‘ë‹µì—ì„œ JSONì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {text[:200]}")

    @staticmethod
    def _extract_first_json_object(text: str) -> dict | None:
        """
        ë¬¸ìì—´ì—ì„œ brace depthë¥¼ ì¶”ì í•˜ì—¬ ì²« ë²ˆì§¸ ì™„ì „í•œ JSON ê°ì²´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        '{ ... } { ... }' í˜•íƒœì—ì„œ ì²« ë²ˆì§¸ë§Œ ê°€ì ¸ì˜´.
        """
        start = text.find("{")
        if start == -1:
            return None

        depth = 0
        in_string = False
        escape = False

        for i in range(start, len(text)):
            ch = text[i]

            if escape:
                escape = False
                continue

            if ch == "\\":
                if in_string:
                    escape = True
                continue

            if ch == '"' and not escape:
                in_string = not in_string
                continue

            if in_string:
                continue

            if ch == "{":
                depth += 1
            elif ch == "}":
                depth -= 1
                if depth == 0:
                    candidate = text[start : i + 1]
                    try:
                        return json.loads(candidate)
                    except json.JSONDecodeError:
                        return None

        return None

    # ------------------------------------------------------------------
    # í…ŒìŠ¤íŠ¸
    # ------------------------------------------------------------------
    async def test_ask(self, question: str):
        """ì—°ê²° í™•ì¸ìš© í…ŒìŠ¤íŠ¸ ë©”ì„œë“œ"""
        try:
            response = await self.llm.ainvoke(
                [
                    SystemMessage(content="ë„ˆëŠ” ì¹œì ˆí•œ ë„ìš°ë¯¸ì•¼."),
                    HumanMessage(content=question),
                ]
            )
            return response.content
        except Exception as e:
            return f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}"


if __name__ == "__main__":

    async def run_test():
        service = AIService()
        print("ğŸ” NVIDIA NIM ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        res = await service.test_ask("Hello, NIM!")
        print(f"ğŸ¤– ì‘ë‹µ: {res}")

    asyncio.run(run_test())
